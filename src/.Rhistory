plot(output_layer2, col = y_inx + 1)
rm(list = ls())
gc(reset = T)
tensorflow::use_python("C:\\ProgramData\\Anaconda3\\python.exe")
if(!require(tensorflow)) install.packages('tensorflow')
require(tensorflow)
set.seed(1)
sample_size = 1000L
x_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
y_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
x = runif(n = sample_size, min = -1, max = 1)
y = runif(n = sample_size, min = -1, max = 1)
toy_data = rbind(cbind(x, x_locat_inx), cbind(y_locat_inx, y))
p = ncol(toy_data)
y_inx = matrix(ifelse(apply(toy_data, 1, function(x) prod(x) >= 0), 1, 0))
plot(toy_data, col = y_inx + 1) ##
x_tf = tf$placeholder(tf$float32, shape(NULL, p))
y_tf = tf$placeholder(tf$float32, shape(NULL, 1))
weight_layer1_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer1_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf)) + bias_layer1_tf)
output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf))
weight_layer2_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer2_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf) + bias_layer2_tf)
output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf))
weight_layer3_tf = tf$Variable(tf$random_normal(shape(p, 1)))
# bias_layer3_tf = tf$Variable(tf$random_normal(shape(1)))
# output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf) + bias_layer3_tf)
output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf))
y_predicted_tf = tf$clip_by_value(output_layer3_tf, clip_value_min = 1e-6, clip_value_max = 1 - 1e-6)
y_inx_predicted_tf = tf$cast(y_predicted_tf >= 0.5, dtype = tf$float32)
accuracy_tf = tf$reduce_mean(tf$cast(tf$equal(y_tf, y_inx_predicted_tf), dtype = tf$float32))
objective_fun = -tf$reduce_mean(y_tf * tf$log(y_predicted_tf) + (1-y_tf) * tf$log(1 - y_predicted_tf))
train_opt = tf$train$AdamOptimizer(learning_rate = 0.005)$minimize(objective_fun)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
max_iter = 5000L
for(iter in 1:max_iter)
{
sess$run(train_opt, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
if(iter %% 1000L == 0)
{
accuracy_val = sess$run(accuracy_tf, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
cat(iter, ' - th iteration loss : ', accuracy_val * 100, '% \n')
}
}
weight_layer1 = sess$run(weight_layer1_tf)
# bias_layer1 = sess$run(bias_layer1_tf)
weight_layer2 = sess$run(weight_layer2_tf)
# bias_layer2 = sess$run(bias_layer2_tf)
output_layer1 = sess$run(output_layer1_tf, feed_dict = dict(x_tf = toy_data))
output_layer2 = sess$run(output_layer2_tf, feed_dict = dict(x_tf = toy_data))
non_zero_inx1 = apply(output_layer1, 1, function(x) any(x != 0))
non_zero_inx2 = apply(output_layer2, 1, function(x) any(x != 0))
all(which(non_zero_inx2) %in% which(non_zero_inx1))
y_inx_predicted = sess$run(y_inx_predicted_tf, feed_dict = dict(x_tf = toy_data))
table(y_inx_predicted, y_inx)
table(non_zero_inx1)
table(non_zero_inx2)
plot(toy_data, col = y_inx + 1)
plot(toy_data, col = y_inx_predicted + 1)
plot(toy_data, col = non_zero_inx1)
plot(toy_data, col = non_zero_inx2)
plot(output_layer1, col = y_inx + 1)
plot(output_layer1, col = y_inx_predicted + 1)
plot(output_layer2, col = y_inx + 1)
plot(output_layer2, col = y_inx_predicted + 1)
rm(list = ls())
gc(reset = T)
tensorflow::use_python("C:\\ProgramData\\Anaconda3\\python.exe")
if(!require(tensorflow)) install.packages('tensorflow')
require(tensorflow)
set.seed(1)
sample_size = 1000L
x_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
y_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
x = runif(n = sample_size, min = -1, max = 1)
y = runif(n = sample_size, min = -1, max = 1)
toy_data = rbind(cbind(x, x_locat_inx), cbind(y_locat_inx, y))
p = ncol(toy_data)
y_inx = matrix(ifelse(apply(toy_data, 1, function(x) prod(x) >= 0), 1, 0))
plot(toy_data, col = y_inx + 1) ##
x_tf = tf$placeholder(tf$float32, shape(NULL, p))
y_tf = tf$placeholder(tf$float32, shape(NULL, 1))
weight_layer1_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer1_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf)) + bias_layer1_tf)
output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf))
weight_layer2_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer2_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf) + bias_layer2_tf)
output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf))
weight_layer3_tf = tf$Variable(tf$random_normal(shape(p, 1)))
# bias_layer3_tf = tf$Variable(tf$random_normal(shape(1)))
# output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf) + bias_layer3_tf)
output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf))
y_predicted_tf = tf$clip_by_value(output_layer3_tf, clip_value_min = 1e-6, clip_value_max = 1 - 1e-6)
y_inx_predicted_tf = tf$cast(y_predicted_tf >= 0.5, dtype = tf$float32)
accuracy_tf = tf$reduce_mean(tf$cast(tf$equal(y_tf, y_inx_predicted_tf), dtype = tf$float32))
objective_fun = -tf$reduce_mean(y_tf * tf$log(y_predicted_tf) + (1-y_tf) * tf$log(1 - y_predicted_tf))
train_opt = tf$train$AdamOptimizer(learning_rate = 0.005)$minimize(objective_fun)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
max_iter = 5000L
for(iter in 1:max_iter)
{
sess$run(train_opt, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
if(iter %% 1000L == 0)
{
accuracy_val = sess$run(accuracy_tf, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
cat(iter, ' - th iteration loss : ', accuracy_val * 100, '% \n')
}
}
weight_layer1 = sess$run(weight_layer1_tf)
# bias_layer1 = sess$run(bias_layer1_tf)
weight_layer2 = sess$run(weight_layer2_tf)
# bias_layer2 = sess$run(bias_layer2_tf)
output_layer1 = sess$run(output_layer1_tf, feed_dict = dict(x_tf = toy_data))
output_layer2 = sess$run(output_layer2_tf, feed_dict = dict(x_tf = toy_data))
non_zero_inx1 = apply(output_layer1, 1, function(x) any(x != 0))
non_zero_inx2 = apply(output_layer2, 1, function(x) any(x != 0))
all(which(non_zero_inx2) %in% which(non_zero_inx1))
y_inx_predicted = sess$run(y_inx_predicted_tf, feed_dict = dict(x_tf = toy_data))
table(y_inx_predicted, y_inx)
table(non_zero_inx1)
table(non_zero_inx2)
plot(toy_data, col = y_inx + 1)
plot(toy_data, col = y_inx_predicted + 1)
plot(toy_data, col = non_zero_inx1)
plot(toy_data, col = non_zero_inx2)
plot(output_layer1, col = y_inx + 1)
plot(output_layer1, col = y_inx_predicted + 1)
plot(output_layer2, col = y_inx + 1)
plot(output_layer2, col = y_inx_predicted + 1)
plot(toy_data, col = y_inx + 1)
plot(toy_data, col = y_inx_predicted + 1)
plot(toy_data, col = non_zero_inx1)
plot(toy_data, col = non_zero_inx2)
plot(output_layer1, col = y_inx + 1)
plot(output_layer1, col = y_inx_predicted + 1)
plot(output_layer2, col = y_inx + 1)
plot(output_layer2, col = y_inx_predicted + 1)
rm(list = ls())
gc(reset = T)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(dplyr)) install.packages('dplyr')
if(!require(randomForest)) install.packages('randomForest')
if(!require(e1071)) install.packages('e1071')
require(dplyr)
require(randomForest)
require(e1071)
load('Rdata/cnn_data.Rdata')
### SVM
svm_model = svm(x = train_x, y = train_y$OPS)
svm_model_predicted = predict(svm_model, test_x)
# train loss
mean((train_y$OPS - svm_model$fitted)^2) # MSE (0.034)
sqrt(sum((train_y$OPS - svm_model$fitted)^2 * train_y$AB )/sum(train_y$AB)) # WRMSE (0.0805)
# test loss
mean((test_y$OPS - svm_model_predicted)^2) # test MSE (0.044)
sqrt(sum((test_y$OPS - svm_model_predicted)^2 * test_y$AB )/sum(test_y$AB)) # test WRMSE (0.11670373)
svm_model = svm(x = train_x, y = train_y$OPS)
svm_model_predicted = predict(svm_model, test_x)
# train loss
mean((train_y$OPS - svm_model$fitted)^2) # MSE (0.034)
sqrt(sum((train_y$OPS - svm_model$fitted)^2 * train_y$AB )/sum(train_y$AB)) # WRMSE (0.0805)
# test loss
mean((test_y$OPS - svm_model_predicted)^2) # test MSE (0.044)
sqrt(sum((test_y$OPS - svm_model_predicted)^2 * test_y$AB )/sum(test_y$AB)) # test WRMSE (0.11670373)
rf_model = randomForest(x = train_x, y = train_y$OPS, ntree = 5000)
rf_model_predicted = predict(rf_model, test_x)
# train loss
mean((train_y$OPS - rf_model$predicted)^2) # MSE (0.0466)
sqrt(sum((train_y$OPS - rf_model$predicted)^2 * train_y$AB )/sum(train_y$AB)) # WRMSE (0.1528)
# test loss
mean((test_y$OPS - rf_model_predicted)^2) # test MSE (0.0437)
sqrt(sum((test_y$OPS - rf_model_predicted)^2 * test_y$AB )/sum(test_y$AB)) # test WRMSE (0.1477631)
load('Rdata/data.Rdata')
rf_model = randomForest(x = train_x, y = train_y$OPS, ntree = 5000)
rf_model_predicted = predict(rf_model, test_x)
# train loss
mean((train_y$OPS - rf_model$predicted)^2) # MSE (0.0466)
sqrt(sum((train_y$OPS - rf_model$predicted)^2 * train_y$AB )/sum(train_y$AB)) # WRMSE (0.1528)
# test loss
mean((test_y$OPS - rf_model_predicted)^2) # test MSE (0.0437)
sqrt(sum((test_y$OPS - rf_model_predicted)^2 * test_y$AB )/sum(test_y$AB)) # test WRMSE (0.1477631)
svm_model = svm(x = train_x, y = train_y$OPS)
svm_model_predicted = predict(svm_model, test_x)
# train loss
mean((train_y$OPS - svm_model$fitted)^2) # MSE (0.034)
sqrt(sum((train_y$OPS - svm_model$fitted)^2 * train_y$AB )/sum(train_y$AB)) # WRMSE (0.0805)
# test loss
mean((test_y$OPS - svm_model_predicted)^2) # test MSE (0.044)
sqrt(sum((test_y$OPS - svm_model_predicted)^2 * test_y$AB )/sum(test_y$AB)) # test WRMSE (0.11670373)
load('Rdata/data.Rdata')
load('Rdata/initial_data.Rdata')
##############################################
######## Random Forest & SVM #################
##############################################
rm(list = ls())
gc(reset = T)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(dplyr)) install.packages('dplyr')
if(!require(randomForest)) install.packages('randomForest')
if(!require(e1071)) install.packages('e1071')
require(dplyr)
require(randomForest)
require(e1071)
load('Rdata/initial_data.Rdata')
rf_model = randomForest(x = train_x, y = train_y$OPS, ntree = 5000)
rf_model_AB = randomForest(x = train_x, y = train_y$AB, ntree = 5000)
# train loss
mean((train_y$OPS - rf_model$predicted)^2) # MSE (0.0466)
mean((train_y$AB - rf_model_AB$predicted)^2)
head(rf_model_AB$predicted)
head(train_y$AB)
rf_model_AB = randomForest(x = train_x, y = train_y$AB, ntree = 5000)
mean((train_y$AB - rf_model_AB$predicted)^2)
##############################################
######## Random Forest & SVM #################
##############################################
rm(list = ls())
gc(reset = T)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(dplyr)) install.packages('dplyr'); require(dplyr)
if(!require(randomForest)) install.packages('randomForest'); require(randomForest)
if(!require(e1071)) install.packages('e1071'); require(e1071)
setwd('D:\\Project\\git\\Predict-KBO-OPS\\src\\Rdata')
load('train_season_y.Rdata')
load('test_season_y.Rdata')
load('train_season_x.Rdata')
load('test_season_y.Rdata')
load('test_season_x.Rdata')
save(train_season_x, train_season_y, test_season_x, test_season_y, file = 'season_data.Rdata')
rm(list = ls())
gc(reset = TRUE)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(tidyverse)) install.packages('tidyverse'); require(tidyverse)
if(!require(data.table)) install.packages('data.table'); require(data.table)
if(!require(tensorflow)) install.packages('tensorflow'); require(tensorflow)
options(warn = -1, tibble.width = Inf)
pre_tmp <- fread('data/Pre_Season_Batter.csv', encoding = 'UTF-8') %>% as.tibble
pre_tmp = fread('data/Pre_Season_Batter.csv', encoding = 'UTF-8') %>% as.tibble
rsb = fread('data/Regular_Season_Batter.csv', encoding = 'UTF-8') %>% as.tibble
dbd_tmp = fread('data/Regular_Season_Batter_Day_by_Day.csv', encoding = 'UTF-8') %>% as.tibble
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(c(1,2,3,7:9,30,10:20), -TB)
head(pre)
head(pre_tmp)
colnames(pre)
colnames(pre_tmp)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year,
AB:H, `2B`:GDP, OPS -TB)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year,
AB:H, `2B`:GDP, OPS, -TB)
colnames(pre_tmp)
colnames(pre)
cc = colnames(pre)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(c(1,2,3,7:9,30,10:20), -TB)
dd = colnames(pre)
cc == dd
cc %in% dd
dd
cc[!cc %in% dd]
dd
cc
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year,
AB:H, `1B`:GDP, -TB)
cc = colnames(pre)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(c(1,2,3,7:9,30,10:20), -TB)
dd = colnames(pre)
cc[!cc %in% dd]
cc
dd
cc %in% dd
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(c(1,2,3,7:9,30,10:20), -TB)
cc = colnames(pre)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year,
AB:H, `1B`:GDP, -TB)
dd = colnames(pre)
cc
dd
pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>% colnames()
cc
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(c(1,2,3,7:9,30,10:20), -TB)
cc = colnames(pre)
cc
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id:year, AB:GDP, -TB)
dd = colnames(pre)
cc
dd
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id:year, AB:GDP, `1B`, -TB)
dd = colnames(pre)
cc
dd
cc %in% dd
everything()
#### 전반기, 후반기 나누기.
everything(dbd_tmp)
###########################################
##### Preprocessing for season_data #######
###########################################
rm(list = ls())
gc(reset = TRUE)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(tidyverse)) install.packages('tidyverse'); require(tidyverse)
if(!require(data.table)) install.packages('data.table'); require(data.table)
rsbd <- fread('data/Regular_Season_Batter_Day_by_Day.csv', encoding = 'UTF-8') %>% as_tibble()
rm(list = ls())
gc(reset = TRUE)
if(Sys.getenv('USERNAME') == 'UOS') setwd('C:\\Users\\UOS\\Desktop\\dacon')
if(Sys.getenv('USERNAME') == 'moon') setwd('D:\\Project\\git\\Predict-KBO-OPS\\src')
if(Sys.getenv('USERNAME') == 'kyucheol') setwd('C:\\Users\\kyucheol\\Dropbox\\dacon')
if(!require(tidyverse)) install.packages('tidyverse'); require(tidyverse)
if(!require(data.table)) install.packages('data.table'); require(data.table)
options(warn = -1, tibble.width = Inf)
pre_tmp = fread('data/Pre_Season_Batter.csv', encoding = 'UTF-8') %>% as.tibble
rsb = fread('data/Regular_Season_Batter.csv', encoding = 'UTF-8') %>% as.tibble
dbd_tmp = fread('data/Regular_Season_Batter_Day_by_Day.csv', encoding = 'UTF-8') %>% as.tibble
#### 프리시즌 데이터
pre = pre_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id:year, AB:GDP, `1B`, -TB)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
colnames(dbd)
colnames(dbd_tmp)
colnames(dbd_tmp) %in% colnames(dbd)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
colnames(dbd)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
colnames(dbd)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
cc = colnames(dbd)
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
dd = colnames(dbd)
cc %in% dd
dd %in% cc
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
criterion = 7.17 # 전반기, 후반기 기준 날짜.
dbd_fh = dbd %>% filter(date <= criterion) # 전반기 데이터
dbd_sh = dbd %>% filter(date > criterion) # 후반기 데이터
#### 전반기, 후반기 데이터 합치기.
colnames(dbd_fh)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
dd = colnames(dbd)
dd
cc
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
dd = colnames(dbd)
dd
dd[7:20]
cc[7:20]
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
dbd_fh = dbd %>% filter(date <= criterion) # 전반기 데이터
colnames(dbd_fh)
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`)
colnames(dbd)
colnames(dbd_tmp)
#### 전반기, 후반기 나누기.
dbd = dbd_tmp %>%
mutate(`1B` = H - `2B` - `3B` - `HR`) %>%
select(batter_id, batter_name, year, date, opposing_team, avg1, AB, R, H, `1B`, everything())
criterion = 7.17 # 전반기, 후반기 기준 날짜.
dbd_fh = dbd %>% filter(date <= criterion) # 전반기 데이터
dbd_sh = dbd %>% filter(date > criterion) # 후반기 데이터
#### 전반기, 후반기 데이터 합치기.
rsb_fh_x = dbd_fh %>% group_by(batter_id, batter_name, year) %>%
summarise_at(colnames(.)[c(7:20)], funs(sum)) %>% ungroup()
head(rsb_fh_x)
rsb_fh_y = dbd_fh %>% group_by(batter_id, batter_name, year) %>%
summarise(OBP = (sum(H) + sum(BB) + sum(HBP))/(sum(AB) + sum(BB) + sum(HBP)),
SLG = (sum(`1B`) + sum(`2B`)*2 + sum(`3B`)*3 + sum(HR)*4)/sum(AB),
AB = sum(AB)) %>%
mutate(OPS = OBP + SLG) %>% ungroup()
rsb_sh_x = dbd_sh %>% group_by(batter_id, batter_name, year) %>%
summarise_at(colnames(.)[7:20], funs(sum)) %>% ungroup()
rsb_sh_y = dbd_sh %>% group_by(batter_id, batter_name, year) %>%
summarise(OBP = (sum(H) + sum(BB) + sum(HBP))/(sum(AB) + sum(BB) + sum(HBP)),
SLG = (sum(`1B`) + sum(`2B`)*2 + sum(`3B`)*3 + sum(HR)*4)/sum(AB),
AB = sum(AB)) %>%
mutate(OPS = OBP + SLG) %>% ungroup()
rm(list = ls())
gc(reset = T)
tensorflow::use_python("C:\\ProgramData\\Anaconda3\\python.exe")
if(!require(tensorflow)) install.packages('tensorflow')
require(tensorflow)
set.seed(1)
sample_size = 1000L
x_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
y_locat_inx = sample(c(-1, 1), size = sample_size, replace = T)
x = runif(n = sample_size, min = -1, max = 1)
y = runif(n = sample_size, min = -1, max = 1)
toy_data = rbind(cbind(x, x_locat_inx), cbind(y_locat_inx, y))
p = ncol(toy_data)
y_inx = matrix(ifelse(apply(toy_data, 1, function(x) prod(x) >= 0), 1, 0))
plot(toy_data, col = y_inx + 1) ##
x_tf = tf$placeholder(tf$float32, shape(NULL, p))
y_tf = tf$placeholder(tf$float32, shape(NULL, 1))
weight_layer1_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer1_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf)) + bias_layer1_tf)
output_layer1_tf = tf$nn$relu(tf$matmul(x_tf, weight_layer1_tf))
weight_layer2_tf = tf$Variable(tf$random_normal(shape(p, p)))
# bias_layer2_tf = tf$Variable(tf$random_normal(shape(p)))
# output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf) + bias_layer2_tf)
output_layer2_tf = tf$nn$relu(tf$matmul(output_layer1_tf, weight_layer2_tf))
weight_layer3_tf = tf$Variable(tf$random_normal(shape(p, 1)))
# bias_layer3_tf = tf$Variable(tf$random_normal(shape(1)))
# output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf) + bias_layer3_tf)
output_layer3_tf = tf$nn$sigmoid(tf$matmul(output_layer2_tf, weight_layer3_tf))
y_predicted_tf = tf$clip_by_value(output_layer3_tf, clip_value_min = 1e-6, clip_value_max = 1 - 1e-6)
y_inx_predicted_tf = tf$cast(y_predicted_tf >= 0.5, dtype = tf$float32)
accuracy_tf = tf$reduce_mean(tf$cast(tf$equal(y_tf, y_inx_predicted_tf), dtype = tf$float32))
objective_fun = -tf$reduce_mean(y_tf * tf$log(y_predicted_tf) + (1-y_tf) * tf$log(1 - y_predicted_tf))
train_opt = tf$train$AdamOptimizer(learning_rate = 0.005)$minimize(objective_fun)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
max_iter = 5000L
for(iter in 1:max_iter)
{
sess$run(train_opt, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
if(iter %% 1000L == 0)
{
accuracy_val = sess$run(accuracy_tf, feed_dict = dict(x_tf = toy_data,
y_tf = y_inx))
cat(iter, ' - th iteration loss : ', accuracy_val * 100, '% \n')
}
}
weight_layer1 = sess$run(weight_layer1_tf)
weight_layer2 = sess$run(weight_layer2_tf)
output_layer1 = sess$run(output_layer1_tf, feed_dict = dict(x_tf = toy_data))
output_layer2 = sess$run(output_layer2_tf, feed_dict = dict(x_tf = toy_data))
non_zero_inx1 = apply(output_layer1, 1, function(x) any(x != 0))
non_zero_inx2 = apply(output_layer2, 1, function(x) any(x != 0))
all(which(non_zero_inx2) %in% which(non_zero_inx1))
y_inx_predicted = sess$run(y_inx_predicted_tf, feed_dict = dict(x_tf = toy_data))
table(y_inx_predicted, y_inx)
plot(toy_data, col = y_inx + 1)
plot(toy_data, col = y_inx_predicted + 1)
plot(toy_data, col = non_zero_inx1)
all(which(non_zero_inx2) %in% which(non_zero_inx1))
plot(toy_data, col = non_zero_inx2)
plot(output_layer1, col = y_inx + 1)
plot(output_layer1, col = y_inx_predicted + 1)
plot(output_layer2, col = y_inx + 1)
plot(output_layer2, col = y_inx_predicted + 1)
plot(output_layer2, col = y_inx + 1)
plot(output_layer2, col = y_inx_predicted + 1)
